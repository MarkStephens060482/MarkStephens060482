{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkStephens060482/MarkStephens060482/blob/main/Big_Data_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmwhGDhnhiUi"
      },
      "source": [
        "# Assignment 2 - PageRank, Frequent Items\n",
        "### Mark Stephens \n",
        "## Exercise 1 - PageRank with MapReduce\n",
        "The algorithm is run on the  Google Web Graph 2002 available at\n",
        "http://snap.stanford.edu/data/web-Google.html "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUZCifvDiY6s"
      },
      "source": [
        "Install pyspark and mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPCsLlk1hNxt",
        "outputId": "b150c144-adda-4a5e-871d-93183554c9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=2dcbc3ee8d2386e14f7e98e2b305014db756b40683505be9f47deacc1e8b8622\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLvII8rTYaOh"
      },
      "source": [
        "Mount the Google Drive to Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZePtPRXmYWWh",
        "outputId": "904b6477-5e5b-4eea-a5fb-b308cbdc0de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLk3J8NpYo_v"
      },
      "source": [
        "Initialise the Spark Session and Spark Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQwREXUgihsi"
      },
      "outputs": [],
      "source": [
        "# Create SparkSession and SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "                  .master('local[*]')\\\n",
        "                  .appName('PageRank')\\\n",
        "                  .getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7q7LVkbiqMh"
      },
      "outputs": [],
      "source": [
        "input_test_PATH = '/content/drive/MyDrive/Colab Notebooks/PangeRank test.txt'\n",
        "input_PATH = '/content/drive/MyDrive/Colab Notebooks/web-Google.txt'\n",
        "GoogleWebGraph = sc.textFile(input_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYrzxd31UT6E"
      },
      "source": [
        "Define a combiner to combine each node and successors. The mapKeys function returns a tuple of the form ((node,([successor_node],value)).The *createCombiner* function passes the key and list tuple to the combineByKey function. The *mergeValue* and *mergeCombiner* functions takes the Value tuple for each node key and appends to the [successor_nodes] list and adds a count to the value. The end result is a total of the out-degree of each node and the list of the node's successors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUtWvbNvSetL"
      },
      "outputs": [],
      "source": [
        "# Defining createCombiner, mergeValue and mergeCombiner functions\n",
        "def mapKeys(tpl):\n",
        "   return (tpl[0],([tpl[1]],1))\n",
        "\n",
        "def createCombiner(value):\n",
        "  return value\n",
        "    \n",
        "def mergeValue(combiner, new_value): \n",
        "  return (combiner[0] + new_value[0],combiner[1]+1)\n",
        "    \n",
        "def mergeCombiner(combiner1, combiner2): \n",
        "  return (combiner1[0] + combiner2[0],combiner1[1] + combiner2[1])\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96QIEapWAHBy"
      },
      "source": [
        "The processing of the Network textfile involves parsing each line and seperating the elements by a tab delimiter, forming an RDD. Filtering out the first four lines of the RDD as they are text and not related to the data. Convert strings to integers. Combine elements of RDD to form sparse representation of the transition matrix of the network. Initialise a page rank RDD made up of initial values of rank for each node based on the reciprocal of number of nodes, and send the RDD vector to cache in preparation to use in preceeding calculations.  Define the values for the damping factor, taxation and number of iterations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEZNIbINi68d"
      },
      "outputs": [],
      "source": [
        "# Preprocessing and initialisaing vectors and parameters\n",
        "#transform the adjacency list text file to split at the occurrences of a tab\n",
        "data = GoogleWebGraph.map(lambda line: line.split(\"\\t\"))\n",
        "\n",
        "# filter the top lines from the resilient distributed dataset (RDD)\n",
        "rdd = data.zipWithIndex().filter(lambda a:a[1]>3).map(lambda a: a[0])\n",
        "\n",
        "#Identify the number of nodes in the network from the text in line 3 of the datafile\n",
        "n = int(data.zipWithIndex().filter(lambda a:a[1]==2).map(lambda a: a[0]).collect()[0][0].split(' ')[2])\n",
        "\n",
        "# map the elements to tuples and convert from strings to integers.\n",
        "rdd = rdd.map(lambda x: tuple(map(int,x)))\n",
        "\n",
        "# map to a key value pair for the first element and form a list with the second element,\n",
        "#achieve a sparse representation of the transition matrix\n",
        "sparserepM = rdd.map(mapKeys).combineByKey(createCombiner,mergeValue,mergeCombiner)\n",
        "\n",
        "# define the intial rank value per node key as the reciprocal of the number of nodes for the whole set.\n",
        "# Each page has equal scores to begin with. \n",
        "vector_zero = sparserepM.map(lambda x: (x[0],1/n))\n",
        "\n",
        "# Set the damping factor and taxation rate\n",
        "damping = 0.80\n",
        "tax = (1-damping)/n\n",
        "\n",
        "# define the maximum number of iterations and convergence tolerance\n",
        "max_iterations = 30\n",
        "tolerance = 10**(-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxtaLxpKNYdw"
      },
      "source": [
        "A Transition matrix of a Webpage network is very sparse. It is better to represent the transition matrix by the out-degree of each node and the list of its successors. The *CombineByKey()* function is used to achieve the sparse representation, as an example below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6n45SSTi7Uu",
        "outputId": "ec140054-f71a-4315-d49f-427f14c70c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+----------+\n",
            "|  node|          neighbours|out-degree|\n",
            "+------+--------------------+----------+\n",
            "|     0|[11342, 824020, 8...|         4|\n",
            "| 11342|[0, 27469, 38716,...|        14|\n",
            "|824020|[0, 91807, 322178...|        11|\n",
            "|203402|[1, 53051, 164684...|        30|\n",
            "|223236|[8517, 14456, 515...|        24|\n",
            "+------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sparserepM.map(lambda x: (x[0],x[1][0],x[1][1])).toDF(schema = ['node','neighbours','out-degree']).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCwREJGeH3s1"
      },
      "source": [
        "Each of the non-zero elements in the Transition matrix, as given by the sparse representation, is mapped to a tuple in the form of (node, successor, probability). The function *map_Matrix()* takes a tuple of the sparse representation of the Transition matrix and returns a list of tuples of the form (node, successor, probability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_OwCcirkAdS"
      },
      "outputs": [],
      "source": [
        "def map_matrix(sparse_tuple):\n",
        "  \"\"\"\n",
        "  The function will take a tuple of the sparse representation of the Transition Matrix and map it to a tuple \n",
        "  of the form (node,successor,probability).\n",
        "  \"\"\"\n",
        "  node = sparse_tuple[0]\n",
        "  successors = sparse_tuple[1][0]\n",
        "  probability = 1/sparse_tuple[1][1]\n",
        "  result=[]\n",
        "  #loop through the list of successors for each node key in the passed tuple\n",
        "  for successor in successors:\n",
        "    result.append((node,(successor,probability)))\n",
        "  return result\n",
        "  \n",
        "# apply the function to each element in the sparse representation RDD to give a flattered list of tuples.\n",
        "#Cache the resulting RDD for efficient processing as matrix is used in preceeding transformations.\n",
        "matrix = sparserepM.flatMap(lambda x: map_matrix(x)).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9kh5XIYUSte"
      },
      "source": [
        "### PageRank Recursion\n",
        "An iteration of the PageRank algorithm involves beginning with an initial estimated PageRank vector $v_{k-1}$ and computing the next estimate $v_{k}$ via the recursive rule:\n",
        "\n",
        "$v_{k}=Œ≤Mùô´_{k-1}+(1-Œ≤){ùêû\\over n}$\n",
        "\n",
        "A matrix-vector multiplication is performed between **M** and $ùêØ_{k-1}$. This is incorporated in a function *matrix_vector()*, which will take a tuple of the form (node,(Mij,vi)). This tuple is formed by joining the matrix, **M**, element and corresponding vector, $ùêØ_{k-1}$, element for each key given by *node*. The function *matrix_vector()* will return the contribution that each node makes to the page rank score of the successor node as a list of tuples of the form (successor,score_interval), where *score_interval* is the interval of the rank score contributed to the successor from the node.\n",
        "\n",
        "The scalar multiplication of $Œ≤$ and vector addition of $(1-Œ≤){ùêû\\over n}$ is performed element-wise using *mapValues()*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLqBWmBmOIdo"
      },
      "outputs": [],
      "source": [
        "def matrix_vector(joined_tuple):\n",
        "  ''' \n",
        "  Performs piecewise matrix vector multiplication of the matrix element and vector element for corresponding 'node' key.\n",
        "  arguement:\n",
        "        tuple: (i, (Mij, vj)), where  Mij is of the form (j, mij) \n",
        "  '''\n",
        "  matrix_element = joined_tuple[1][0]\n",
        "  rank_score = joined_tuple[1][1]\n",
        "  result = []\n",
        "  # Unpack the matrix element tuple\n",
        "  successor, probability = matrix_element\n",
        "  #perform pievewise multiplication to obtain page_rank_part, the interval of rank score contributed to the successor from node.\n",
        "  result.append((successor, rank_score*probability))  \n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6c2g8hx1Wbh"
      },
      "source": [
        "The following function calculates the squared difference of an array vector. It takes a partitioned rdd and forms a numpy array to then perform the squared difference from rdd element values. This is used to test convergence of the iteration procedure of the PageRank algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V_3A2Dg1Vt3"
      },
      "outputs": [],
      "source": [
        "def calculate_L2_norm(tpl):\n",
        "  array_vector = np.array([(x[1][0]-x[1][1])**2 for x in tpl])\n",
        "  return array_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m59xDY9A4n0s"
      },
      "source": [
        "The main program sequence consists of a iterative loop to evaluate and update the page rank vector and is as follows:\n",
        "1. define the initial vector $ùì•_{0}$ as the vector object to be updated.  \n",
        "2. join the transition matrix, $M$, RDD and the vector ùí± RDD and send to cache memory.\n",
        "3. Perform the matrix vector multiplication as a map job of elements-wise multiplications then add up all elements of the same row in the reduce job.\n",
        "4. Map the values of the matrix vector multiplication to perform scalar multiplication by Œ≤ and add the taxation amount to each element.\n",
        "5. Perform convergence test by calculating the L2 norm of the difference of the new rdd vector and the previous rdd vector and compare against the tolerance. \n",
        "6. Assign this new calculated page rank estimate to the vector object and repeat the process until convergence is satisfied or the specified maximum number of iterations is achieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwEePoVDRCsK",
        "outputId": "2c80d079-d681-425f-ea63-2ba178722fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The algorithm converged on the 25 iteration with a L2 norm value of 7.960493110838393e-08.\n"
          ]
        }
      ],
      "source": [
        "# *********The Main Program***********\n",
        "import math, numpy as np\n",
        "\n",
        "if __name__ == \"__main__\": \n",
        "  prev_vector = None\n",
        "  i = 0\n",
        "  #Loop through the number of iterations\n",
        "  while i < max_iterations:\n",
        "    i += 1\n",
        "    if prev_vector == None:\n",
        "      #assign the vector v to the initial vector v0 on the first iteration and save in memory to use later.\n",
        "      prev_vector = vector_zero.persist()\n",
        "    # join the matrix elements with corresponding vector elements acvcording to node key forming (node,(Mij,vj))\n",
        "    matrixvectorjoin = matrix.join(prev_vector).cache()\n",
        "    # produce an RDD 'score_interval' that contains the contributions of each source node to the rank score of its successor nodes.\n",
        "    # The score_interval\n",
        "    score_interval = matrixvectorjoin.flatMap(lambda joined_tuple: matrix_vector(joined_tuple))\n",
        "    # The score intervals for each successor nodes are added together via the reducer task.\n",
        "    # This completes the Matrix Vector multiplication.\n",
        "    score_contributions = score_interval.reduceByKey(lambda x, y: x + y)\n",
        "    # The vector_new estimate for each node is achieved by multiplying by the damping and adding on the tax amount\n",
        "    vector_new = score_contributions.mapValues(lambda x: damping*x +  tax)\n",
        "    # save the result of the new rdd vector in the memory to use it further in testing convergence.\n",
        "    vector_new.cache()  \n",
        "    # convergence between the new updated page rank vector and the previous is tested using L2 norm of the two vectors and comparing to the tolerance\n",
        "    l2_norm = vector_new.join(prev_vector).mapPartitions(calculate_L2_norm).sum()**0.5\n",
        "   \n",
        "    #Check for convergence\n",
        "    if l2_norm < tolerance:\n",
        "      print(f\"The algorithm converged on the {i} iteration with a L2 norm value of {l2_norm}.\")\n",
        "      break\n",
        "    # remove the previous rdd from memory\n",
        "    prev_vector.unpersist()\n",
        "    # update the rdd vector object and save in memory to use later. \n",
        "    prev_vector = vector_new.persist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ49LXI5Vb7y"
      },
      "source": [
        "Having performed the recursive calculations of the page rank vector, the elements of the vector RDD are mapped to a tab delimited formatted output. This is applied to each partition of the rdd in parallel. The RDD is then materialised as a textfile and saved to drive.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSiqaSpfiKwy"
      },
      "outputs": [],
      "source": [
        "# Produce the output in the format <node><TAB><Page Rank>, employing mapPartitions to apply the lambda function to each partition\n",
        "output = vector_new.sortByKey().mapPartitions(lambda partitions: (\"%s\\t%s\" %(x[0],\"{:.4E}\".format(x[1])) for x in partitions))\n",
        "\n",
        "#headers for the output file are defined\n",
        "header_rdd = sc.parallelize([\"nodes  pagerank\"])\n",
        "\n",
        "#The output page rank for each node and the headers for the output file are combined using union.\n",
        "output_with_headers = header_rdd.union(output)\n",
        "\n",
        "# define the directory where the partition files are located\n",
        "dir_path = \"/content/drive/MyDrive/Colab Notebooks/pagerankoutput\"\n",
        "\n",
        "# write the RDD output to a HDFS coalesced to a single partition file.\n",
        "output_with_headers.coalesce(1,True).saveAsTextFile(dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoLdpbJgx0IQ"
      },
      "source": [
        "The Output of the Top 10 nodes by page rank score is given below. This is achieved by first sorting the RDD page rank vector in descending order, formatting the page rank values to scientific notation so the number form is more presentable and converting the RDD to a dataframe, showing only the top 10 results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMtRwL1OVcEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d776b8-148f-47ca-8032-f2cd6cbbff0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+\n",
            "|  node| page rank|\n",
            "+------+----------+\n",
            "|558791|1.7488E-05|\n",
            "| 41909|1.5670E-05|\n",
            "|425770|1.5507E-05|\n",
            "| 32163|1.5172E-05|\n",
            "|828963|1.4194E-05|\n",
            "|504140|1.3961E-05|\n",
            "|751384|1.2749E-05|\n",
            "|486980|1.2295E-05|\n",
            "|597621|1.2115E-05|\n",
            "|605856|1.1904E-05|\n",
            "+------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Having achieved the maximum number of iterations the top nodes with page rank scores are reported. The rdd is sorted by page rank value.\n",
        "vector_new = vector_new.sortBy(lambda x: x[1], ascending = False)\n",
        "# the page rank values are formated and the top 10 results are given as a dataframe.\n",
        "vector_new.map(lambda x: (x[0],\"{:.4E}\".format(x[1]))).toDF(schema = ['node','page rank']).show(10) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zawOVBPYihLg"
      },
      "source": [
        "The Binary File with name *part-00000* is converted to a textfile with extension .txt such that it may be read externally by any text file reader application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys7WWXbvijtN"
      },
      "outputs": [],
      "source": [
        "# import the os and shutil libraries\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# loop over all files in the directory and rename them with the .txt extension\n",
        "for filename in os.listdir(dir_path):\n",
        "    if filename.startswith('part-'):\n",
        "        os.rename(os.path.join(dir_path, filename), os.path.join(dir_path, filename + '.txt'))\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdEUGAt26QZC"
      },
      "source": [
        "***\n",
        "## Exercise 2 - Frequent Itemsets\n",
        "\n",
        "## Simple, randomized A-Priori algorithm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhjY6ONdsMbW"
      },
      "source": [
        "An implementation of the simple randomised A-priori algorithm is performed and tested on [frequent itemset mining dataset](http://fimi.uantwerpen.be/data/) found in the repository. The data is preprocessed by seperating the items by whitespace. A random sample of the data is taken, shuffling the dataset before hand. The support threshold proportion of the total number of transactions, $s$, is defined and is reduced to $0.9s$ for the sample of transactions. This allows identifying in the sample almost all itemsets having support of atleast $s$ in the whole dataset.\n",
        "\n",
        "The *get_frequent_itemsets()* function determines determines a list of itemsets that are frequent as defined by the given support threshold , $s$.\n",
        "\n",
        "The *generate_candidate_itemsets()* function returns a list of all combinations of itemsets of a given size $k$.\n",
        "\n",
        "The *prune_candidates()* function reduced the candidate itemset list based on the apriori principle.\n",
        "\n",
        "The *sample_transactions()* function takes a random sample of the whole dataset for a given sample size proportion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82Q_Iyj3o4g4"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "import random\n",
        "\n",
        "def get_frequent_itemsets(transactions, itemsets, min_support):\n",
        "  \"\"\"\n",
        "  Determine the frequent items and their support\n",
        "  arguments:\n",
        "  transactions: list[list] A list of lists of transactions\n",
        "  itemsets: [list] A list of candidate itemsets\n",
        "  min_support: float, The minimum support for frequent items\n",
        "  \"\"\"\n",
        "  frequent_itemsets = []\n",
        "  \n",
        "  itemset_counts={}\n",
        "  # number of transactions\n",
        "  T = len(transactions)\n",
        "\n",
        "  for itemset in itemsets:\n",
        "         \n",
        "    itemset_counts[itemset] = 0\n",
        "        \n",
        "    for transaction in transactions:\n",
        "      if set(itemset).issubset(transaction):\n",
        "        itemset_counts[itemset] += 1\n",
        "\n",
        "  # sort the dictionary of itemset and support count  by support count value in descending order.\n",
        "  itemset_counts = dict(sorted(itemset_counts.items(), key=lambda x:x[1], reverse=True ))\n",
        "  \n",
        "  #Add the frequent itemsets to the list based on suppor to define a frequent item\n",
        "  for itemset,count in itemset_counts.items():\n",
        "    if count >= min_support*T:\n",
        "      frequent_itemsets.append(itemset)\n",
        "  \n",
        "  return frequent_itemsets\n",
        "\n",
        "def generate_candidate_itemsets(itemsets,k):\n",
        "  \"\"\"\n",
        "  Generates all possible candidate itemsets of size k from the given itemsets, finding possible combinations.\n",
        "  \"\"\"\n",
        "  candidates = list(combinations(itemsets,k))\n",
        "  return sorted(candidates)\n",
        "\n",
        "def prune_candidates(candidates,frequent_itemsets):\n",
        "  \"\"\"\n",
        "  Prunes candidates itemsets based on the apriori principle.\n",
        "  arguements:\n",
        "  candidates: list of candidates itemsets\n",
        "  frequent_itemsets:   set of frequent itemsets \n",
        "  \"\"\"\n",
        "  list_of_set = [set(x) for x in frequent_itemsets]\n",
        "  pruned_candidates = []\n",
        "  for candidate in candidates:\n",
        "    is_pruned = False\n",
        "    # check if all subsets of candidate are frequent\n",
        "    for e in candidate:\n",
        "      subset = set(candidate).difference(set([e]))\n",
        "          \n",
        "      if subset not in list_of_set:\n",
        "        is_pruned = True\n",
        "        break\n",
        "    if not is_pruned:\n",
        "      pruned_candidates.append(candidate)\n",
        "  return pruned_candidates\n",
        "\n",
        "def sample_transactions(transactions, sample_probability,randomise):\n",
        "  '''\n",
        "  Take a random sample of transactions of sampling proportion of p. For m transactions there will be a\n",
        "  sample size of m.p transactions. If the list of transactions are randomised already, then select the first m.p samples instead of reading \n",
        "  the whole dataset.\n",
        "  arguements:\n",
        "  transactions: List[list].  A list of transactions.\n",
        "  sample_probability: Float.  A sampling proportion, p.\n",
        "  randomise: Bool.  The sample should be randomised.    \n",
        "  '''\n",
        "  m = len(transactions)\n",
        "  sample_size = round(m * sample_probability)\n",
        "  if randomise:\n",
        "    sample = random.sample(transactions, sample_size)\n",
        "  else:\n",
        "    sample = transactions[:sample_size]\n",
        "  return sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *apriori_algorithm()* function initially forms a list of distinct itemsets of size 1 from scanning all transactions of the dataset. The support counts for each of these items is found and the frequent items, items whose support count is greater than the threshold, are retained in a frequent items list. From the list of frequent itemsets of size 1, candidate itemsets of size 2 are found by listing possible combinations. The list of candidate itemsets of size 2 are pruned using the apriori principle, that is, cannot be frequent if a subset of the itemset is not frequent themselves.The pruned candiated itemset list is then counted in all transactions and are retained as frequent itemsets if they meet the support count threshold. This process is repeated for all k sized itemsets."
      ],
      "metadata": {
        "id": "6pxjhcutDtVC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JMwQpRq6PVx"
      },
      "outputs": [],
      "source": [
        "def apriori_algorithm(transactions, min_support):\n",
        "    \"\"\"\n",
        "    Returns all frequent itemsets in the given transactions using the Apriori algorithm.\n",
        "    transactions: List of sets where each set represents a transaction\n",
        "    min_support: Minimum support threshold for an itemset to be considered frequent as a proportion of the total number of transactions.\n",
        "    \"\"\"\n",
        "    itemsets = []\n",
        "    # Find the unique items present in the transactions\n",
        "    for transaction in transactions:\n",
        "      for item in transaction:\n",
        "        if item not in itemsets:\n",
        "          itemsets.append(item)\n",
        "    \n",
        "    # Sort the itemsets in ascending order\n",
        "    itemsets = sorted(itemsets)\n",
        "    \n",
        "    # Initialize the frequent itemsets\n",
        "    frequent_itemsets = []\n",
        "    \n",
        "    # Get the frequent itemsets of size 1\n",
        "    frequent_itemsets.append(sorted(get_frequent_itemsets(transactions, itemsets, min_support)))\n",
        " \n",
        "    k = 2\n",
        "    while len(frequent_itemsets[k-2]) > 0:\n",
        "      # Generate candidate itemsets of size k\n",
        "      candidate_itemsets = generate_candidate_itemsets(frequent_itemsets[0], k)\n",
        "      \n",
        "      # prune the candidate itemsets\n",
        "      pruned_candidate_itemsets = prune_candidates(candidate_itemsets, frequent_itemsets[k-2])\n",
        "     \n",
        "      # Get the frequent itemsets of size k\n",
        "      frequent_itemsets_k = get_frequent_itemsets(transactions, pruned_candidate_itemsets, min_support)\n",
        "      \n",
        "      if len(frequent_itemsets_k) > 0:\n",
        "        frequent_itemsets.append(frequent_itemsets_k) \n",
        "        k += 1\n",
        "      else:\n",
        "        break\n",
        "    return frequent_itemsets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *apriori algorithm* is performed on a simple randomised sample of the whole set of transations from multiple datasets. The algorithm is performed for various sample sizes. Having achieved all frequent itemsets up to size k from a given sample, the whole dataset is passed through once and the support is determined for the frequent itemsets. The itemsets that have support greater than the threshold are retained while those that are frequent in the sample but not in the whole dataset are *false positives* and are removed.\n",
        "\n",
        "The Apriori algorithm is repeated across 7 different datasets for 3 different samples sizes of 2%, 5% and 10%. The support threshold is also set either at 1% or 5% according to the particular dataset. A list of the top 10 frequent itemsets per itemset size is produced as the output for each dataset and for each sample. The number of False Positives that were removed is also stated ans well as the computation time."
      ],
      "metadata": {
        "id": "uZfVWtw7Gxat"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQl9hcQwVAlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbdc57ea-c964-4b14-e20d-bfce796bd715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset is: connect.dat.txt\n",
            "\n",
            "Sample size is: 0.1 and support ratio threshold of 0.2\n",
            "frequent itemsets of size 1 :['114', '51', '17', '14', '47']\n",
            "\t\t\t['5', '74', '33', '15', '111']\n",
            "frequent itemsets of size 2 :[('1', '4'), ('4', '7'), ('1', '5'), ('1', '7')]\n",
            "\t\t\t[]\n",
            "frequent itemsets of size 3 :[('1', '4', '7')]\n",
            "\t\t\t[]\n",
            "The number of False positives removed from frequent itemset are: 4\n",
            "The computation time was: 8.21\n",
            "\n",
            "Sample size is: 0.2 and support ratio threshold of 0.2\n",
            "frequent itemsets of size 1 :['114', '51', '17', '14', '47']\n",
            "\t\t\t['5', '74', '33', '15', '111']\n",
            "frequent itemsets of size 2 :[('1', '4'), ('4', '7'), ('1', '5'), ('1', '7')]\n",
            "\t\t\t[]\n",
            "frequent itemsets of size 3 :[('1', '4', '7')]\n",
            "\t\t\t[]\n",
            "The number of False positives removed from frequent itemset are: 0\n",
            "The computation time was: 10.98\n",
            "\n",
            "The dataset is: mushroom.dat.txt\n",
            "\n",
            "Sample size is: 0.1 and support ratio threshold of 0.07\n",
            "frequent itemsets of size 1 :['69', '22', '93', '91', '2']\n",
            "\t\t\t['116', '17', '13', '9', '113']\n",
            "frequent itemsets of size 2 :[('1', '3'), ('1', '7'), ('2', '9'), ('1', '6'), ('2', '3')]\n",
            "\t\t\t[('6', '9'), ('2', '6'), ('3', '9'), ('1', '9')]\n",
            "frequent itemsets of size 3 :[('1', '3', '9')]\n",
            "\t\t\t[]\n",
            "The number of False positives removed from frequent itemset are: 1\n",
            "The computation time was: 1.13\n",
            "\n",
            "Sample size is: 0.2 and support ratio threshold of 0.07\n",
            "frequent itemsets of size 1 :['69', '22', '93', '91', '2']\n",
            "\t\t\t['116', '17', '13', '9', '113']\n",
            "frequent itemsets of size 2 :[('1', '3'), ('1', '7'), ('2', '9'), ('1', '6'), ('2', '3')]\n",
            "\t\t\t[('6', '9'), ('2', '6'), ('3', '9'), ('1', '9')]\n",
            "frequent itemsets of size 3 :[('1', '3', '9')]\n",
            "\t\t\t[]\n",
            "The number of False positives removed from frequent itemset are: 1\n",
            "The computation time was: 0.97\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# import the builtin time module\n",
        "import time\n",
        "\n",
        "files = [#'T40I10D100K.dat.txt',\n",
        "         #'T10I4D100K.dat.txt',\n",
        "         #'chess.dat.txt',\n",
        "         'connect.dat.txt',\n",
        "         'mushroom.dat.txt']\n",
        "         #'pumsb.dat.txt',\n",
        "         #'pumsb_star.dat.txt']\n",
        "\n",
        "####### MAIN PROGRAM ###########\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  #loop through all files\n",
        "  for i,f in enumerate(files):\n",
        "    # open the datafile\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/apriori algorithm/' + f,encoding='utf8') as dataFile:\n",
        "    # preprocess the transaction data\n",
        "      transactions = []\n",
        "      for line in dataFile.read().split('\\n'):\n",
        "        transactions.append(line.split())\n",
        "              \n",
        "    new_line = '\\n'\n",
        "    print(f'The dataset is: {f}{new_line}')\n",
        "\n",
        "    #sample sizes of the transactions\n",
        "    sample_sizes = [0.1,0.2]#[0.02,0.05,0.1]\n",
        "    for prob in sample_sizes:\n",
        "      # Grab Currrent Time Before Running the Code\n",
        "      start = time.time()\n",
        "    \n",
        "      sample = sample_transactions(transactions, prob,randomise = True)\n",
        "\n",
        "      # define the support thresholds as porportions of total transactions for each of the 7 dataset.\n",
        "      support_threshold_proportions = [0.2, 0.07] #[0.01, 0.01, 0.7, 0.2, 0.07, 0.01, 0.01]\n",
        "      \n",
        "      # identify as having support at least 0.9 * s in the sample almost all those itemsets\n",
        "      # that have support at least s is the whole\n",
        "      sample_support = 0.9 * support_threshold_proportions[i]\n",
        "\n",
        "      #Perform the A-priori Algorithm and collect frequent itemsets of all itemset sizes for the sample support.\n",
        "      frequent_itemsets_sample = apriori_algorithm(sample,sample_support)\n",
        "            \n",
        "      # Identifying False Positives by passing through the whole set of transactions and counting the occurrences\n",
        "      # of the frequent itemsets found in the sample for the support of s.\n",
        "      false_positives = set()\n",
        "      \n",
        "      frequent_itemsets_filtered = {}\n",
        "      for j,frequent_itemset in enumerate(frequent_itemsets_sample):\n",
        "        # Have the list of frequent itemsets from the whole dataset returned with support counts.\n",
        "        pop_frequent_itemset = get_frequent_itemsets(transactions,\n",
        "                                                     frequent_itemset,\n",
        "                                                     support_threshold_proportions[i])\n",
        "        \n",
        "        false_positives.update(set(frequent_itemset).difference(set(pop_frequent_itemset)))\n",
        "        \n",
        "        # Itemsets found to be frequent in the sample but not frequent in the whole dataset are removed from\n",
        "        # the frequent itemsets list of the sample.\n",
        "        # elements of the frequent itemsets taken from the sample that are also frequent in the whole set are retained. \n",
        "        frequent_itemsets_filtered[j+1] = list(set(frequent_itemset).intersection(set(pop_frequent_itemset)))\n",
        "      # Grab Currrent Time After Running the Code\n",
        "      end = time.time()\n",
        "\n",
        "      #Subtract Start Time from The End Time\n",
        "      total_time = end - start  \n",
        "      # The top 10 most freqent itemssets for each size are given in the output\n",
        "      new_line = '\\n'\n",
        "      tab = '\\t'\n",
        "      print(f'Sample size is: {prob} and support ratio threshold of {support_threshold_proportions[i]}')\n",
        "\n",
        "      for k,v in frequent_itemsets_filtered.items():\n",
        "        if len(v) > 0:\n",
        "          print(f'frequent itemsets of size {k} :{v[:5]}{new_line}{tab}{tab}{tab}{v[5:10]}')\n",
        "      print(f'The number of False positives removed from frequent itemset are: {len(false_positives)}')\n",
        "      print(f'The computation time was: {round(total_time,2)}{new_line}') \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Algorithm of Savasere, Omiecinski, and Navathe\n",
        "This is an improvement on the simple randomised algorithm  avoiding both false negatives and false positives as a result of frequent itemset mining on just a sample of the wholed ataset. As a cost for the accurate result, SON algorithm makes two passes of the whole dataset.This can be computationally achieveable by partitioning the dataset as a distributed file system and compute the A-priori algorithm in parallel on each partition. The process is implemented in MapReduce  and consists of two Mapping and Reduction phases."
      ],
      "metadata": {
        "id": "pZeh-_I2Ox0d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azkgEk--4YqV"
      },
      "outputs": [],
      "source": [
        "# Create SparkSession and SparkContext\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "findspark.init()\n",
        "spark = SparkSession.builder\\\n",
        "                  .master('local[*]')\\\n",
        "                  .appName('Frequent Itemset SON algorithm')\\\n",
        "                  .getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *first_Mapper()* function performs the Apriori algorithm on the allocated chunk of the dataset and returns a list of candidate frequent itemsets. This is performed for each chunk, or partition, of the dataset.\n",
        "\n",
        "The *second_Mapper()* function takes the whole reduced candidate frequent itemset list from all partitions and the support count is determined for each partition. A list of the candidate frequent itemsets and support counts is returned."
      ],
      "metadata": {
        "id": "t_rkhCwG2J0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def first_Mapper(partitions, min_support):\n",
        "  \"\"\"\n",
        "  Performs the first map job by performing the A-priori algorithm on each partition and returns\n",
        "  a list of candidate frequent itemsets seperately.\n",
        "  \"\"\"\n",
        "  transactions = list(partitions)\n",
        "  # Perform local A-priori algorithm on chunks\n",
        "  candidate_itemsets_apriori = apriori_algorithm(transactions, min_support)\n",
        "  candidate_itemsets_list = [(x,1) for sublist in candidate_itemsets_apriori for x in sublist]\n",
        "  return iter(candidate_itemsets_list)\n",
        "\n",
        "def second_Mapper(partitions,itemsets):\n",
        "  \"\"\"\n",
        "  Determine the frequent items and their support.\n",
        "  arguments:\n",
        "  transactions: list[list] A list of transaction in a chunk\n",
        "  itemsets: A list of candidate itemsets\n",
        "  \"\"\"\n",
        "  itemset_counts={}\n",
        "  transactions = list(partitions)\n",
        "  for itemset in itemsets.value:\n",
        "    itemset_counts[itemset] = 0\n",
        "    for transaction in transactions:     \n",
        "      if set(itemset).issubset(transaction):\n",
        "        itemset_counts[itemset] += 1\n",
        "    \n",
        "  itemset_counts_iter = [(k,v) for k,v in itemset_counts.items()]\n",
        "  return iter(itemset_counts_iter)\n",
        "\n",
        "def length(element):\n",
        "  '''\n",
        "  Determines the length of the itemset: returning a 1 for a singleton,\n",
        "  2 for a doubleton and a 3 for a tripleton.\n",
        "  '''\n",
        "  if type(element) == str:\n",
        "    return len([element])\n",
        "  return len(element)"
      ],
      "metadata": {
        "id": "TpeFUzXh7RV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SON algorithm consists of the two phases of MapReduce jobs. For each dataset and for varying sample sizes, the SON algorithm is performed and top 10 frequent itemsets that may occur for each itemset size is produced asd the output.  The number of partitions is determined from ${1\\over p}$ where $p$ is the sample size proportion. For instance, the sample size of 10% would result in 10 partitions of the dataset. The *first_Mapper()* function is mapped across all partitions and then the reducer job adds up all the elements of the same key. The support count value is dropped and the remaining list of candidate frequent itemsets is materialised. The *second_Mapper()* function is mapped across all partitions, taking the list of candidate frequent itemsets as an argument. The candidate frequent itemsets and their support count are combined from all partitions. The frequent itemsets are determined from the candidates by filtering out those whom support count is greater than the minimum support threshold. The frequent itemsets are sorted in descending order of support count and materialised into a list. The output is produced showing the top 10 frequent itemsets per size for each dataset and for varying sample size.  "
      ],
      "metadata": {
        "id": "WbzONvx43qDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "files = [#'T40I10D100K.dat.txt',\n",
        "         #'T10I4D100K.dat.txt',\n",
        "         #'chess.dat.txt',\n",
        "         'connect.dat.txt',\n",
        "         'mushroom.dat.txt']#,\n",
        "         #'pumsb.dat.txt',\n",
        "         #'pumsb_star.dat.txt']\n",
        "\n",
        "####### MAIN PROGRAM ###########\n",
        "if __name__ == \"__main__\":\n",
        "  #loop through all files\n",
        "  for i,f in enumerate(files):\n",
        "    #load the dataset\n",
        "    data = sc.textFile('/content/drive/MyDrive/Colab Notebooks/apriori algorithm/' + f)\n",
        "\n",
        "    # split the rdd elements by whitespace and have elements of items in list per transaction \n",
        "    rdd = data.map(lambda line: line.split()).cache()\n",
        "      \n",
        "    # define the support thresholds as porportions of total transactions for each of the 7 dataset.\n",
        "    support_threshold_proportions = [0.2, 0.07] #[0.01, 0.01, 0.7, 0.2, 0.07, 0.01, 0.01]\n",
        "\n",
        "    new_line = '\\n'\n",
        "    print(f'The dataset is: {f}{new_line}')\n",
        "  \n",
        "    #sample sizes of the transactions\n",
        "    partition_proportion =[0.1,0.2] # [0.02,0.05,0.1]\n",
        "    for proportion in partition_proportion:\n",
        "      \n",
        "      # Grab Currrent Time Before Running the Code\n",
        "      start = time.time()\n",
        "\n",
        "      # Define the number of chunks to partition the dataset based on the sample size.\n",
        "      chunk_number = int(1/proportion)\n",
        "      chunks = rdd.repartition(chunk_number).persist()\n",
        "\n",
        "      #total number of transactions\n",
        "      total = chunks.count()\n",
        "\n",
        "      #First Map Reduce job\n",
        "      # Apply the A-priori algorithm on each chunk and Identify Candidate pairs they occor once or more than once \n",
        "      chunk_frequent_itemsets = chunks.mapPartitions(lambda chunk: first_Mapper(chunk,support_threshold_proportions[i]))\n",
        "      # Combine frequent itemsets from chunks to form candidate itemsets\n",
        "      candidate_itemsets = chunk_frequent_itemsets.reduceByKey(lambda x,y: x+y).map(lambda x:x[0]).cache()\n",
        "      candidate_itemsets = candidate_itemsets.collect()\n",
        "        \n",
        "      #broadcast the itemsets data to each worker in distribution environment.\n",
        "      candidate_itemsets = sc.broadcast(candidate_itemsets)\n",
        "      # candidate_itemsets.value to retrieve the data list.\n",
        "\n",
        "      # Second Map Reduce job\n",
        "      #The Map tasks for the second Map function take all the  candidate itemsets and a chunk of the input data file.\n",
        "      # Each Map task counts the number of occurrences of each of the candidate itemsets among the transaction in the chunk of the dataset.\n",
        "      # The output is a set of key-value pairs (C,v) where C is the candidate itemset and v is the support.\n",
        "      itemsets_support = chunks.mapPartitions(lambda chunk: second_Mapper(chunk,candidate_itemsets))\n",
        "\n",
        "      # Add the key value pairs and filter itemsets based on support.\n",
        "      frequent_itemset = itemsets_support.reduceByKey(lambda x, y: x + y)\\\n",
        "      .filter(lambda x: x[1] >= math.ceil(support_threshold_proportions[i]*total))\n",
        "\n",
        "      #Sort the frequent item list based on support counts and cache the rdd ready to materialise.\n",
        "      frequent_itemset_sorted = frequent_itemset.sortBy(lambda x: x[1], ascending = False).cache()\n",
        "\n",
        "      #materialise the frequent itemset list.\n",
        "      frequent_itemset_sorted = frequent_itemset_sorted.collect()\n",
        "      \n",
        "      #unpersist the original partitioned rdd of transactions from main memory.\n",
        "      chunks.unpersist()\n",
        "\n",
        "      # Organise the list of frequent itemsets into a dictionary of itemsets of varying sizes.\n",
        "      freq_list ={(j+1):[e[0] for e in frequent_itemset_sorted if length(e[0]) == (j+1)] for j in range(3)}\n",
        "\n",
        "      # Grab Currrent Time After Running the Code\n",
        "      end = time.time()\n",
        "\n",
        "      #Subtract Start Time from The End Time\n",
        "      total_time = end - start  \n",
        "\n",
        "      # The top 10 most freqent itemsets for each size are given in the output\n",
        "      new_line = '\\n'\n",
        "      tab = '\\t'\n",
        "      print(f'Sample size is {proportion} with support ratio threshold of {support_threshold_proportions[i]}')\n",
        "      \n",
        "      # Loop through key and value of the frequent itemset dictionary and print results.\n",
        "      for k,v in freq_list.items():\n",
        "        if len(v) > 0:\n",
        "          print(f'frequent itemsets of size {k} :{v[:5]}{new_line}{tab}{tab}{tab}{v[5:10]}')\n",
        "      print(f'The computation time was: {round(total_time,2)}{new_line}') "
      ],
      "metadata": {
        "id": "LdMwYzDEOl6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddfe2060-7841-4d9f-a950-348c7bbb001d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset is: connect.dat.txt\n",
            "\n",
            "Sample size is 0.1 with support ratio threshold of 0.2\n",
            "frequent itemsets of size 1 :['1', '111', '11', '77', '7']\n",
            "\t\t\t['71', '117', '17', '4', '74']\n",
            "frequent itemsets of size 2 :[('1', '7'), ('4', '7'), ('1', '4'), ('1', '5')]\n",
            "\t\t\t[]\n",
            "frequent itemsets of size 3 :[('1', '4', '7')]\n",
            "\t\t\t[]\n",
            "The computation time was: 28.63\n",
            "\n",
            "Sample size is 0.2 with support ratio threshold of 0.2\n",
            "frequent itemsets of size 1 :['1', '11', '111', '7', '77']\n",
            "\t\t\t['117', '17', '71', '47', '44']\n",
            "frequent itemsets of size 2 :[('1', '7'), ('4', '7'), ('1', '4'), ('1', '5')]\n",
            "\t\t\t[]\n",
            "frequent itemsets of size 3 :[('1', '4', '7')]\n",
            "\t\t\t[]\n",
            "The computation time was: 24.75\n",
            "\n",
            "The dataset is: mushroom.dat.txt\n",
            "\n",
            "Sample size is 0.1 with support ratio threshold of 0.07\n",
            "frequent itemsets of size 1 :['2', '22', '1', '111', '11']\n",
            "\t\t\t['33', '3', '6', '66', '99']\n",
            "frequent itemsets of size 2 :[('2', '3'), ('1', '3'), ('2', '6'), ('1', '6'), ('1', '9')]\n",
            "\t\t\t[('2', '9'), ('3', '9'), ('6', '9'), ('1', '7')]\n",
            "frequent itemsets of size 3 :[('1', '3', '9')]\n",
            "\t\t\t[]\n",
            "The computation time was: 7.61\n",
            "\n",
            "Sample size is 0.2 with support ratio threshold of 0.07\n",
            "frequent itemsets of size 1 :['2', '22', '1', '11', '111']\n",
            "\t\t\t['3', '33', '6', '66', '99']\n",
            "frequent itemsets of size 2 :[('2', '3'), ('1', '3'), ('2', '6'), ('1', '6'), ('1', '9')]\n",
            "\t\t\t[('2', '9'), ('3', '9'), ('6', '9'), ('1', '7')]\n",
            "frequent itemsets of size 3 :[('1', '3', '9')]\n",
            "\t\t\t[]\n",
            "The computation time was: 3.69\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The challenges in implementation of the above algorithms include having the support count in the outcome of the frequent itemsets for the Simple Randomised Apriori algorithm and failing to avoid the computationally expensive materialisation of RDDs for the SON algorithm. For the simple randomised algorithm, instances of the frequent itemsets identified from the sample were counted in the whole dataset in order to identify False Positives and have them removed from the list. In doing this, the support count was not included as the value together with the frequent itemset key in a key-value tuple pair element. This was due to Python Set datatype operations being employed to find the difference in sets of frequent itemsets from the sample and the whole dataset. The result of this led to the output list of frequent itemsets of different size not being sorted in ascending order by support count. The other challenge in implementation was for the SON algorithm with having to tolerate computationally expensive materialisation of RDDs for two occasions within the algorithm. The candidate frequent itemset RDD obtained from the first MapReduce job had to be materialised to a list inorder for instances of each candidate frequent itemset to be counted in each partition of the dataset. The second occasion of materialising an RDD was after the second MapReduce job to produce the output of the final frequent itemsets for the whole dataset.   \n"
      ],
      "metadata": {
        "id": "wD4lSRg3fypB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "## Exercise 3: Hierarchical Clustering\n",
        "\n",
        "A cluster of points is determined from their distance metric being the minimum. A cluster is represented by its centroid and the distance metric is determined between all other clusters. Two clusters that share a minimum distance between their centroids shall merge and form a new cluster. The $distance()$ function is the euclidean distance metric that for any two one-dimensional point values, reduces to the absolute value of the difference between the two values.  \n",
        "The first part ofthe algorithm is to initialise each point as a cluster and form a list of these clusters. The algorithm repeatedly will form clusters until the stopping criterion is met, which in this scenario is when only a single cluster remains. For each cluster, the distance between centroids is found with every cluster and the two clusters with the smallest distance will merge."
      ],
      "metadata": {
        "id": "YWDsXZ_jkQxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# define a function to compute the distance between two clusters\n",
        "def distance1(c1, c2):\n",
        "  '''\n",
        "  Finds the distance between two clusters from their respective centroids.\n",
        "  '''\n",
        "  return np.abs(np.mean(c1) - np.mean(c2))\n",
        "\n",
        "# Stopping Criteria of specified number of clusters, threshold cluster distance\n",
        "# and single cluster.\n",
        "def cluster_number(min_dist,clusters,threshold):\n",
        "  '''\n",
        "  Fixed number of clusters. This criterion specifies a fixed number of clusters\n",
        "  that the algorithm should produce.\n",
        "  '''\n",
        "  number = len(clusters)\n",
        "  if number <= threshold:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def threshold_distance(min_dist,clusters,threshold):\n",
        "  '''\n",
        "  This criterion specifies a threshold distance above which the clusters should \n",
        "  not be moerged. Once all remaining pairwise distances exceed this threshold,\n",
        "  the algorithm stops.\n",
        "  '''\n",
        "  if min_dist > threshold:\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def single_cluster(min_dist,clusters,threshold = 1):\n",
        "  '''\n",
        "  This is the default stopping criterion of obtaining a single cluster.\n",
        "  '''\n",
        "  return False\n",
        "\n",
        "def hierarchical_clustrering(data,distance,stopping_criterion,threshold):\n",
        "  '''\n",
        "  Performs hierarchical clustering with given distance metric, stopping criterion and threshold.\n",
        "  '''\n",
        "  # initialize the clusters\n",
        "  clusters = [[x] for x in data]\n",
        "  print(clusters)\n",
        "  iteration = 0\n",
        "  # calculate distance between all clusters and identify the minimum distance\n",
        "  while len(clusters) > 1:\n",
        "    iteration += 1\n",
        "    min_dist = np.inf\n",
        "    for i in range(len(clusters)):\n",
        "        for j in range(i+1, len(clusters)):\n",
        "            d = distance(clusters[i], clusters[j])\n",
        "            \n",
        "            # identify the clusters that have the smallest distance.\n",
        "            if d < min_dist:\n",
        "              c1, c2 = clusters[i], clusters[j]\n",
        "              min_dist = d\n",
        "    \n",
        "    # Stopping Criterion\n",
        "    if stopping_criterion(min_dist,clusters,threshold):\n",
        "      break\n",
        "\n",
        "    # Merging clusters to form new cluster.\n",
        "    clusters.remove(c1)\n",
        "    clusters.remove(c2)\n",
        "    clusters.append(c1 + c2)\n",
        "    \n",
        "    #print the resulting clusters and their centroids.\n",
        "    for i,cluster in enumerate(clusters):\n",
        "      centroid = np.mean(cluster)\n",
        "      if i == 0:\n",
        "        print(f'Iteration number: {iteration} ')\n",
        "      print(f'Cluster {i+1}: {cluster} with centroid of {centroid}')\n"
      ],
      "metadata": {
        "id": "0hkT6u3-Jdag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical clustering is performed for a one-dimensional array of values. The default distance metric of absolute difference between centroids is used and the outcome of the algorithm is compared for 3 conditions of stopping criterion: single cluster, threshold distance and specific number of clusters "
      ],
      "metadata": {
        "id": "dty3z4lcC_QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # define the data points\n",
        "data = np.array([1,4,11,16,25,36,49,64,81])\n",
        "\n",
        "#stopping criteria to perform th Hierarchical clustering algorithm one at a time\n",
        "stopping_criteria = [cluster_number,threshold_distance,single_cluster]\n",
        "\n",
        "#the corresponding threshold hyperparameters foreach of the above stopping criterion\n",
        "thresholds = [2, 25, 1] \n",
        "\n",
        "new_line = '\\n'\n",
        "# Perform hierarchical clustering\n",
        "for i,stopping_criterion in enumerate(stopping_criteria):\n",
        "  print(f'{new_line}***Hierarchical Clustering for stopping criterion of {stopping_criterion.__name__}\\\n",
        "   with threshold of {thresholds[i]}{new_line}***')\n",
        "  hierarchical_clustrering(data,distance1,stopping_criterion, thresholds[i])\n"
      ],
      "metadata": {
        "id": "pWStOtgtC-bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9d44cc-8d70-48aa-97a2-80072ec9ee7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "***Hierarchical Clustering for stopping criterion of cluster_number  with threshold of 2\n",
            "***\n",
            "[[1], [4], [11], [16], [25], [36], [49], [64], [81]]\n",
            "Iteration number: 1 \n",
            "Cluster 1: [11] with centroid of 11.0\n",
            "Cluster 2: [16] with centroid of 16.0\n",
            "Cluster 3: [25] with centroid of 25.0\n",
            "Cluster 4: [36] with centroid of 36.0\n",
            "Cluster 5: [49] with centroid of 49.0\n",
            "Cluster 6: [64] with centroid of 64.0\n",
            "Cluster 7: [81] with centroid of 81.0\n",
            "Cluster 8: [1, 4] with centroid of 2.5\n",
            "Iteration number: 2 \n",
            "Cluster 1: [25] with centroid of 25.0\n",
            "Cluster 2: [36] with centroid of 36.0\n",
            "Cluster 3: [49] with centroid of 49.0\n",
            "Cluster 4: [64] with centroid of 64.0\n",
            "Cluster 5: [81] with centroid of 81.0\n",
            "Cluster 6: [1, 4] with centroid of 2.5\n",
            "Cluster 7: [11, 16] with centroid of 13.5\n",
            "Iteration number: 3 \n",
            "Cluster 1: [49] with centroid of 49.0\n",
            "Cluster 2: [64] with centroid of 64.0\n",
            "Cluster 3: [81] with centroid of 81.0\n",
            "Cluster 4: [1, 4] with centroid of 2.5\n",
            "Cluster 5: [11, 16] with centroid of 13.5\n",
            "Cluster 6: [25, 36] with centroid of 30.5\n",
            "Iteration number: 4 \n",
            "Cluster 1: [49] with centroid of 49.0\n",
            "Cluster 2: [64] with centroid of 64.0\n",
            "Cluster 3: [81] with centroid of 81.0\n",
            "Cluster 4: [25, 36] with centroid of 30.5\n",
            "Cluster 5: [1, 4, 11, 16] with centroid of 8.0\n",
            "Iteration number: 5 \n",
            "Cluster 1: [81] with centroid of 81.0\n",
            "Cluster 2: [25, 36] with centroid of 30.5\n",
            "Cluster 3: [1, 4, 11, 16] with centroid of 8.0\n",
            "Cluster 4: [49, 64] with centroid of 56.5\n",
            "Iteration number: 6 \n",
            "Cluster 1: [81] with centroid of 81.0\n",
            "Cluster 2: [49, 64] with centroid of 56.5\n",
            "Cluster 3: [25, 36, 1, 4, 11, 16] with centroid of 15.5\n",
            "Iteration number: 7 \n",
            "Cluster 1: [25, 36, 1, 4, 11, 16] with centroid of 15.5\n",
            "Cluster 2: [81, 49, 64] with centroid of 64.66666666666667\n",
            "\n",
            "***Hierarchical Clustering for stopping criterion of threshold_distance  with threshold of 25\n",
            "***\n",
            "[[1], [4], [11], [16], [25], [36], [49], [64], [81]]\n",
            "Iteration number: 1 \n",
            "Cluster 1: [11] with centroid of 11.0\n",
            "Cluster 2: [16] with centroid of 16.0\n",
            "Cluster 3: [25] with centroid of 25.0\n",
            "Cluster 4: [36] with centroid of 36.0\n",
            "Cluster 5: [49] with centroid of 49.0\n",
            "Cluster 6: [64] with centroid of 64.0\n",
            "Cluster 7: [81] with centroid of 81.0\n",
            "Cluster 8: [1, 4] with centroid of 2.5\n",
            "Iteration number: 2 \n",
            "Cluster 1: [25] with centroid of 25.0\n",
            "Cluster 2: [36] with centroid of 36.0\n",
            "Cluster 3: [49] with centroid of 49.0\n",
            "Cluster 4: [64] with centroid of 64.0\n",
            "Cluster 5: [81] with centroid of 81.0\n",
            "Cluster 6: [1, 4] with centroid of 2.5\n",
            "Cluster 7: [11, 16] with centroid of 13.5\n",
            "Iteration number: 3 \n",
            "Cluster 1: [49] with centroid of 49.0\n",
            "Cluster 2: [64] with centroid of 64.0\n",
            "Cluster 3: [81] with centroid of 81.0\n",
            "Cluster 4: [1, 4] with centroid of 2.5\n",
            "Cluster 5: [11, 16] with centroid of 13.5\n",
            "Cluster 6: [25, 36] with centroid of 30.5\n",
            "Iteration number: 4 \n",
            "Cluster 1: [49] with centroid of 49.0\n",
            "Cluster 2: [64] with centroid of 64.0\n",
            "Cluster 3: [81] with centroid of 81.0\n",
            "Cluster 4: [25, 36] with centroid of 30.5\n",
            "Cluster 5: [1, 4, 11, 16] with centroid of 8.0\n",
            "Iteration number: 5 \n",
            "Cluster 1: [81] with centroid of 81.0\n",
            "Cluster 2: [25, 36] with centroid of 30.5\n",
            "Cluster 3: [1, 4, 11, 16] with centroid of 8.0\n",
            "Cluster 4: [49, 64] with centroid of 56.5\n",
            "Iteration number: 6 \n",
            "Cluster 1: [81] with centroid of 81.0\n",
            "Cluster 2: [49, 64] with centroid of 56.5\n",
            "Cluster 3: [25, 36, 1, 4, 11, 16] with centroid of 15.5\n",
            "Iteration number: 7 \n",
            "Cluster 1: [25, 36, 1, 4, 11, 16] with centroid of 15.5\n",
            "Cluster 2: [81, 49, 64] with centroid of 64.66666666666667\n",
            "\n",
            "***Hierarchical Clustering for stopping criterion of single_cluster  with threshold of 1\n",
            "***\n",
            "[[1], [4], [11], [16], [25], [36], [49], [64], [81]]\n",
            "Iteration number: 1 \n",
            "Cluster 1: [11] with centroid of 11.0\n",
            "Cluster 2: [16] with centroid of 16.0\n",
            "Cluster 3: [25] with centroid of 25.0\n",
            "Cluster 4: [36] with centroid of 36.0\n",
            "Cluster 5: [49] with centroid of 49.0\n",
            "Cluster 6: [64] with centroid of 64.0\n",
            "Cluster 7: [81] with centroid of 81.0\n",
            "Cluster 8: [1, 4] with centroid of 2.5\n",
            "Iteration number: 2 \n",
            "Cluster 1: [25] with centroid of 25.0\n",
            "Cluster 2: [36] with centroid of 36.0\n",
            "Cluster 3: [49] with centroid of 49.0\n",
            "Cluster 4: [64] with centroid of 64.0\n",
            "Cluster 5: [81] with centroid of 81.0\n",
            "Cluster 6: [1, 4] with centroid of 2.5\n",
            "Cluster 7: [11, 16] with centroid of 13.5\n",
            "Iteration number: 3 \n",
            "Cluster 1: [49] with centroid of 49.0\n",
            "Cluster 2: [64] with centroid of 64.0\n",
            "Cluster 3: [81] with centroid of 81.0\n",
            "Cluster 4: [1, 4] with centroid of 2.5\n",
            "Cluster 5: [11, 16] with centroid of 13.5\n",
            "Cluster 6: [25, 36] with centroid of 30.5\n",
            "Iteration number: 4 \n",
            "Cluster 1: [49] with centroid of 49.0\n",
            "Cluster 2: [64] with centroid of 64.0\n",
            "Cluster 3: [81] with centroid of 81.0\n",
            "Cluster 4: [25, 36] with centroid of 30.5\n",
            "Cluster 5: [1, 4, 11, 16] with centroid of 8.0\n",
            "Iteration number: 5 \n",
            "Cluster 1: [81] with centroid of 81.0\n",
            "Cluster 2: [25, 36] with centroid of 30.5\n",
            "Cluster 3: [1, 4, 11, 16] with centroid of 8.0\n",
            "Cluster 4: [49, 64] with centroid of 56.5\n",
            "Iteration number: 6 \n",
            "Cluster 1: [81] with centroid of 81.0\n",
            "Cluster 2: [49, 64] with centroid of 56.5\n",
            "Cluster 3: [25, 36, 1, 4, 11, 16] with centroid of 15.5\n",
            "Iteration number: 7 \n",
            "Cluster 1: [25, 36, 1, 4, 11, 16] with centroid of 15.5\n",
            "Cluster 2: [81, 49, 64] with centroid of 64.66666666666667\n",
            "Iteration number: 8 \n",
            "Cluster 1: [25, 36, 1, 4, 11, 16, 81, 49, 64] with centroid of 31.88888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of finding the distance between clusters as measured from their centroid, the distance can be determined as the minimum distance between any two points within the respective clusters. The implication on heirarchical clistering is shown below:"
      ],
      "metadata": {
        "id": "2uwqj9CUMbfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distance2(c1, c2):\n",
        "  '''\n",
        "  Finds the distances between all points in two clusters and returns the minimum \n",
        "  as the distance between the two clusters.\n",
        "  '''\n",
        "  distances = []\n",
        "  # Find the distances between each point in cluster 1 and every other point in cluster 2\n",
        "  for p1 in c1:\n",
        "    for p2 in c2:\n",
        "      d = np.abs(p1-p2)\n",
        "      distances.append(d)\n",
        "  \n",
        "  # determine the minimum of the distances as the distance between the two clusters.\n",
        "  d_min = min(distances)\n",
        "  return d_min"
      ],
      "metadata": {
        "id": "so8nb5grMXFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hierarchical Clustering algorithm is again performed with using the second distance metric. The outcomes are compared across the different stopping metrics."
      ],
      "metadata": {
        "id": "KVQkejwpRVof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the data points\n",
        "data = np.array([1,4,11,16,25,36,49,64,81])\n",
        "\n",
        "#stopping criteria to perform th Hierarchical clustering algorithm one at a time\n",
        "stopping_criteria = [cluster_number,threshold_distance,single_cluster]\n",
        "\n",
        "#the corresponding threshold hyperparameters foreach of the above stopping criterion\n",
        "thresholds = [2, 15, 1] \n",
        "\n",
        "new_line = '\\n'\n",
        "# Perform hierarchical clustering\n",
        "for i,stopping_criterion in enumerate(stopping_criteria):\n",
        "  print(f'{new_line}***Hierarchical Clustering for stopping criterion of {stopping_criterion.__name__}\\\n",
        "   with threshold of {thresholds[i]}{new_line}***')\n",
        "  # The alternate distance metric is used in the Hierarchical Clustering.\n",
        "  hierarchical_clustrering(data,distance2,stopping_criterion, thresholds[i])"
      ],
      "metadata": {
        "id": "PXZ0khzfRWAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85306c33-b738-4986-8915-33bb62c6f6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "***Hierarchical Clustering for stopping criterion of cluster_number   with threshold of 2\n",
            "***\n",
            "[[1], [4], [11], [16], [25], [36], [49], [64], [81]]\n",
            "Iteration number: 1 \n",
            "Cluster 1: [11] with centroid of 11.0\n",
            "Cluster 2: [16] with centroid of 16.0\n",
            "Cluster 3: [25] with centroid of 25.0\n",
            "Cluster 4: [36] with centroid of 36.0\n",
            "Cluster 5: [49] with centroid of 49.0\n",
            "Cluster 6: [64] with centroid of 64.0\n",
            "Cluster 7: [81] with centroid of 81.0\n",
            "Cluster 8: [1, 4] with centroid of 2.5\n",
            "Iteration number: 2 \n",
            "Cluster 1: [25] with centroid of 25.0\n",
            "Cluster 2: [36] with centroid of 36.0\n",
            "Cluster 3: [49] with centroid of 49.0\n",
            "Cluster 4: [64] with centroid of 64.0\n",
            "Cluster 5: [81] with centroid of 81.0\n",
            "Cluster 6: [1, 4] with centroid of 2.5\n",
            "Cluster 7: [11, 16] with centroid of 13.5\n",
            "Iteration number: 3 \n",
            "Cluster 1: [25] with centroid of 25.0\n",
            "Cluster 2: [36] with centroid of 36.0\n",
            "Cluster 3: [49] with centroid of 49.0\n",
            "Cluster 4: [64] with centroid of 64.0\n",
            "Cluster 5: [81] with centroid of 81.0\n",
            "Cluster 6: [1, 4, 11, 16] with centroid of 8.0\n",
            "Iteration number: 4 \n",
            "Cluster 1: [36] with centroid of 36.0\n",
            "Cluster 2: [49] with centroid of 49.0\n",
            "Cluster 3: [64] with centroid of 64.0\n",
            "Cluster 4: [81] with centroid of 81.0\n",
            "Cluster 5: [25, 1, 4, 11, 16] with centroid of 11.4\n",
            "Iteration number: 5 \n",
            "Cluster 1: [49] with centroid of 49.0\n",
            "Cluster 2: [64] with centroid of 64.0\n",
            "Cluster 3: [81] with centroid of 81.0\n",
            "Cluster 4: [36, 25, 1, 4, 11, 16] with centroid of 15.5\n",
            "Iteration number: 6 \n",
            "Cluster 1: [64] with centroid of 64.0\n",
            "Cluster 2: [81] with centroid of 81.0\n",
            "Cluster 3: [49, 36, 25, 1, 4, 11, 16] with centroid of 20.285714285714285\n",
            "Iteration number: 7 \n",
            "Cluster 1: [81] with centroid of 81.0\n",
            "Cluster 2: [64, 49, 36, 25, 1, 4, 11, 16] with centroid of 25.75\n",
            "\n",
            "***Hierarchical Clustering for stopping criterion of threshold_distance   with threshold of 15\n",
            "***\n",
            "[[1], [4], [11], [16], [25], [36], [49], [64], [81]]\n",
            "Iteration number: 1 \n",
            "Cluster 1: [11] with centroid of 11.0\n",
            "Cluster 2: [16] with centroid of 16.0\n",
            "Cluster 3: [25] with centroid of 25.0\n",
            "Cluster 4: [36] with centroid of 36.0\n",
            "Cluster 5: [49] with centroid of 49.0\n",
            "Cluster 6: [64] with centroid of 64.0\n",
            "Cluster 7: [81] with centroid of 81.0\n",
            "Cluster 8: [1, 4] with centroid of 2.5\n",
            "Iteration number: 2 \n",
            "Cluster 1: [25] with centroid of 25.0\n",
            "Cluster 2: [36] with centroid of 36.0\n",
            "Cluster 3: [49] with centroid of 49.0\n",
            "Cluster 4: [64] with centroid of 64.0\n",
            "Cluster 5: [81] with centroid of 81.0\n",
            "Cluster 6: [1, 4] with centroid of 2.5\n",
            "Cluster 7: [11, 16] with centroid of 13.5\n",
            "Iteration number: 3 \n",
            "Cluster 1: [25] with centroid of 25.0\n",
            "Cluster 2: [36] with centroid of 36.0\n",
            "Cluster 3: [49] with centroid of 49.0\n",
            "Cluster 4: [64] with centroid of 64.0\n",
            "Cluster 5: [81] with centroid of 81.0\n",
            "Cluster 6: [1, 4, 11, 16] with centroid of 8.0\n",
            "Iteration number: 4 \n",
            "Cluster 1: [36] with centroid of 36.0\n",
            "Cluster 2: [49] with centroid of 49.0\n",
            "Cluster 3: [64] with centroid of 64.0\n",
            "Cluster 4: [81] with centroid of 81.0\n",
            "Cluster 5: [25, 1, 4, 11, 16] with centroid of 11.4\n",
            "Iteration number: 5 \n",
            "Cluster 1: [49] with centroid of 49.0\n",
            "Cluster 2: [64] with centroid of 64.0\n",
            "Cluster 3: [81] with centroid of 81.0\n",
            "Cluster 4: [36, 25, 1, 4, 11, 16] with centroid of 15.5\n",
            "Iteration number: 6 \n",
            "Cluster 1: [64] with centroid of 64.0\n",
            "Cluster 2: [81] with centroid of 81.0\n",
            "Cluster 3: [49, 36, 25, 1, 4, 11, 16] with centroid of 20.285714285714285\n",
            "Iteration number: 7 \n",
            "Cluster 1: [81] with centroid of 81.0\n",
            "Cluster 2: [64, 49, 36, 25, 1, 4, 11, 16] with centroid of 25.75\n",
            "\n",
            "***Hierarchical Clustering for stopping criterion of single_cluster   with threshold of 1\n",
            "***\n",
            "[[1], [4], [11], [16], [25], [36], [49], [64], [81]]\n",
            "Iteration number: 1 \n",
            "Cluster 1: [11] with centroid of 11.0\n",
            "Cluster 2: [16] with centroid of 16.0\n",
            "Cluster 3: [25] with centroid of 25.0\n",
            "Cluster 4: [36] with centroid of 36.0\n",
            "Cluster 5: [49] with centroid of 49.0\n",
            "Cluster 6: [64] with centroid of 64.0\n",
            "Cluster 7: [81] with centroid of 81.0\n",
            "Cluster 8: [1, 4] with centroid of 2.5\n",
            "Iteration number: 2 \n",
            "Cluster 1: [25] with centroid of 25.0\n",
            "Cluster 2: [36] with centroid of 36.0\n",
            "Cluster 3: [49] with centroid of 49.0\n",
            "Cluster 4: [64] with centroid of 64.0\n",
            "Cluster 5: [81] with centroid of 81.0\n",
            "Cluster 6: [1, 4] with centroid of 2.5\n",
            "Cluster 7: [11, 16] with centroid of 13.5\n",
            "Iteration number: 3 \n",
            "Cluster 1: [25] with centroid of 25.0\n",
            "Cluster 2: [36] with centroid of 36.0\n",
            "Cluster 3: [49] with centroid of 49.0\n",
            "Cluster 4: [64] with centroid of 64.0\n",
            "Cluster 5: [81] with centroid of 81.0\n",
            "Cluster 6: [1, 4, 11, 16] with centroid of 8.0\n",
            "Iteration number: 4 \n",
            "Cluster 1: [36] with centroid of 36.0\n",
            "Cluster 2: [49] with centroid of 49.0\n",
            "Cluster 3: [64] with centroid of 64.0\n",
            "Cluster 4: [81] with centroid of 81.0\n",
            "Cluster 5: [25, 1, 4, 11, 16] with centroid of 11.4\n",
            "Iteration number: 5 \n",
            "Cluster 1: [49] with centroid of 49.0\n",
            "Cluster 2: [64] with centroid of 64.0\n",
            "Cluster 3: [81] with centroid of 81.0\n",
            "Cluster 4: [36, 25, 1, 4, 11, 16] with centroid of 15.5\n",
            "Iteration number: 6 \n",
            "Cluster 1: [64] with centroid of 64.0\n",
            "Cluster 2: [81] with centroid of 81.0\n",
            "Cluster 3: [49, 36, 25, 1, 4, 11, 16] with centroid of 20.285714285714285\n",
            "Iteration number: 7 \n",
            "Cluster 1: [81] with centroid of 81.0\n",
            "Cluster 2: [64, 49, 36, 25, 1, 4, 11, 16] with centroid of 25.75\n",
            "Iteration number: 8 \n",
            "Cluster 1: [81, 64, 49, 36, 25, 1, 4, 11, 16] with centroid of 31.88888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By changing the distance metric to the minimum of distances of corresponding points from two clusters, results in a Hierarchical cLustering process that merges a cluster and the very next closest point. This process continues until the stopping criterion is satisfied and results in different outcomes compared to the hierarchical clustering under the centroid-based distance metric. In particular, the clustering under the threshold distance stopping criterion requires a lower distance threshold to achieve two distinct clusters for distance metric 2 compared to distance metric 1. "
      ],
      "metadata": {
        "id": "ittSDYytXGhH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHBZ6KRSmqAxiGiecMSlRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}